{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install all dependancies and libraries"
      ],
      "metadata": {
        "id": "BUKjZDKisuE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4As_e2zts2aG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sinanazeri/build_own_chatbot_without_open_ai.git\n",
        "!mv build_own_chatbot_without_open_ai build_chatbot_for_your_data\n",
        "!cd build_chatbot_for_your_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "AWk1k2utvYsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ibm-watson-machine-learning"
      ],
      "metadata": {
        "id": "VZlUX5L07HLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "GUwx_UB87-Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-ibm"
      ],
      "metadata": {
        "id": "M-cuQ9sHB3OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Chatbot\n",
        "\n",
        "*   From langchain.chains import LLMChain, SimpleSequentialChain\n",
        "*   Initialize the global variables, language model and its embeddings\n",
        "*   Function to process a PDF document\n",
        "*   Build the QA chain, which utilizes the LLM and retriever for answering questions.\n",
        "*   Similarity search is used for retrieval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjbqkcQMszFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
        "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
        "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watson_machine_learning.foundation_models import Model\n",
        "from langchain import PromptTemplate\n",
        "# Check for GPU availability and set the appropriate device for computation.\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Global variables\n",
        "conversation_retrieval_chain = None\n",
        "chat_history = []\n",
        "llm_hub = None\n",
        "embeddings = None\n",
        "# Function to initialize the language model and its embeddings\n",
        "def init_llm():\n",
        "    global llm_hub, embeddings\n",
        "    my_credentials = {\n",
        "\"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n",
        "}\n",
        "    params = {\n",
        "            GenParams.MAX_NEW_TOKENS: 256, # The maximum number of tokens that the model can generate in a single run.\n",
        "            GenParams.TEMPERATURE: 0.1,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
        "        }\n",
        "    LLAMA2_model = Model(\n",
        "            model_id= 'meta-llama/llama-2-70b-chat',\n",
        "            credentials=my_credentials,\n",
        "            params=params,\n",
        "            project_id=\"skills-network\"\n",
        "            )\n",
        "    llm_hub = WatsonxLLM(model=LLAMA2_model)\n",
        "\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": DEVICE}\n",
        "    )\n",
        "\n",
        "def process_document(document_path):\n",
        "    global conversation_retrieval_chain\n",
        "    # Load the document\n",
        "    loader = PyPDFLoader(document_path)\n",
        "    documents = loader.load()\n",
        "    # Split the document into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    # embeddings database using Chroma from the split text chunks.\n",
        "    db = Chroma.from_documents(texts, embedding=embeddings)\n",
        "\n",
        "    conversation_retrieval_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm_hub,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 6, 'lambda_mult': 0.25}),\n",
        "        return_source_documents=False,\n",
        "        input_key = \"question\"\n",
        "    )\n",
        "# Function to process a user prompt\n",
        "def process_prompt(prompt):\n",
        "    global conversation_retrieval_chain\n",
        "    global chat_history\n",
        "    # Query the model\n",
        "    output = conversation_retrieval_chain({\"question\": prompt, \"chat_history\": chat_history})\n",
        "    answer = output[\"result\"]\n",
        "    # Update the chat history\n",
        "    chat_history.append((prompt, answer))\n",
        "    # Return the model\\'s response\n",
        "    return answer\n",
        "# Initialize the language model\n",
        "init_llm()"
      ],
      "metadata": {
        "id": "PrCoVrMV6-nI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}