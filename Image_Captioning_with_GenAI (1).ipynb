{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install dependancies and libraries"
      ],
      "metadata": {
        "id": "AEobAEgkqxvi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc7m1-h_qMnV"
      },
      "outputs": [],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gTHFr36Tqu18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def greet(name, intensity):\n",
        "  return \"Hello, \" + name + \"!\" * int(intensity)\n",
        "demo = gr.Interface(\n",
        "  fn=greet,\n",
        "  inputs=[\"text\", \"slider\"],\n",
        "  outputs=[\"text\"],\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "hbCBET1Zqeju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "F1-aIHkIsef3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for caption generation using the PIL image"
      ],
      "metadata": {
        "id": "aq1UUSlRq7bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "def generate_caption(image):\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs)\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def caption_image(image):\n",
        "    \"\"\"\n",
        "    Takes a PIL Image input and returns a caption.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        caption = generate_caption(image)\n",
        "        return caption\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=caption_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Image Captioning with BLIP\",\n",
        "    description=\"Upload an image to generate a caption.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "7g_D1-glsoPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()"
      ],
      "metadata": {
        "id": "UoaIXH2UtB7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Download human-readable labels for ImageNet.\n",
        "response = requests.get(\"https://git.io/JJkYN\")\n",
        "labels = response.text.split(\"\\n\")\n",
        "\n",
        "def predict(inp):\n",
        " inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
        " with torch.no_grad():\n",
        "  prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
        "  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n",
        " return confidences"
      ],
      "metadata": {
        "id": "yZcyO6whtNAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "gr.Interface(fn=predict,\n",
        "       inputs=gr.Image(type=\"pil\"),\n",
        "       outputs=gr.Label(num_top_classes=3),\n",
        "       examples=[\"/content/lion.jpg\", \"/content/cheetah.jpg\"]).launch()"
      ],
      "metadata": {
        "id": "DmPnvgkwtjl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing required libraries in my_env\n",
        "!pip install langchain==0.1.11 gradio==4.21.0 transformers==4.38.2 bs4==0.0.2 requests==2.31.0 torch==2.2.1"
      ],
      "metadata": {
        "id": "l5dYf5LcwDH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the pretrained processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
      ],
      "metadata": {
        "id": "3VruGuIDxHsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image\n",
        "img_path = \"/content/cars.jpeg\"\n",
        "# convert it into an RGB format\n",
        "image = Image.open(img_path).convert('RGB')"
      ],
      "metadata": {
        "id": "vHwcW0bIBnYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"the image of\"\n",
        "inputs = processor(images=image, text=text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "CpHyUZl2CYnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a caption for the image\n",
        "outputs = model.generate(**inputs, max_length=50)"
      ],
      "metadata": {
        "id": "ms8dWEUPCgmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated tokens to text\n",
        "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "# Print the caption\n",
        "print(caption)"
      ],
      "metadata": {
        "id": "jsPkopNSCp9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "demo.launch(server_name=\"0.0.0.0\", server_port= 7860)\n"
      ],
      "metadata": {
        "id": "1OTKqvxLElEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers Pillow\n"
      ],
      "metadata": {
        "id": "uCuRiv3UE0pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "def caption_image(input_image: np.ndarray):\n",
        "    # Convert numpy array to PIL Image and convert to RGB\n",
        "    raw_image = Image.fromarray(input_image).convert('RGB')\n",
        "\n",
        "    return caption\n",
        "iface = gr.Interface(\n",
        "    fn=caption_image,\n",
        "    inputs=gr.Image(),\n",
        "    outputs=\"text\",\n",
        "    title=\"Image Captioning\",\n",
        "    description=\"This is a simple web app for generating captions for images using a trained model.\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "8otqYUhkE7iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the pretrained processor and model\n",
        "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# URL of the page to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/IBM\"\n",
        "\n",
        "# Download the page\n",
        "response = requests.get(url)\n",
        "# Parse the page with BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all img elements\n",
        "img_elements = soup.find_all('img')\n",
        "\n",
        "# Open a file to write the captions\n",
        "with open(\"captions.txt\", \"w\") as caption_file:\n",
        "    # Iterate over each img element\n",
        "    for img_element in img_elements:\n",
        "        img_url = img_element.get('src')\n",
        "\n",
        "        # Skip if the image is an SVG or too small\n",
        "        if 'svg' in img_url or '1x1' in img_url:\n",
        "            continue\n",
        "\n",
        "        # Correct the URL if it's malformed\n",
        "        if img_url.startswith('//'):\n",
        "            img_url = 'https:' + img_url\n",
        "        elif not img_url.startswith('http://') and not img_url.startswith('https://'):\n",
        "            continue  # Skip URLs that don't start with http:// or https://\n",
        "\n",
        "        try:\n",
        "            # Download the image\n",
        "            response = requests.get(img_url)\n",
        "            # Convert the image data to a PIL Image\n",
        "            raw_image = Image.open(BytesIO(response.content))\n",
        "            if raw_image.size[0] * raw_image.size[1] < 400:  # Skip very small images\n",
        "                continue\n",
        "\n",
        "            raw_image = raw_image.convert('RGB')\n",
        "\n",
        "            # Process the image\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "            # Generate a caption for the image\n",
        "            out = model.generate(**inputs, max_new_tokens=50)\n",
        "            # Decode the generated tokens to text\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # Write the caption to the file\n",
        "            caption_file.write(f\"{img_url}: {caption}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_url}: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "BWJvSVe0GKkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}